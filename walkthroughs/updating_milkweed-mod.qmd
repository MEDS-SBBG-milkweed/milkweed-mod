# Intro


#### Load packages
```{r}
library(spocc)
library(spThin)
library(dismo)
library(sf)
library(ENMeval)
library(wallace)
library(tidyverse)
library(here)
library(terra)
library(raster)
library(gt)
source("R/addLegend_decreasing.R")
source("R/setup.R")
```


```{r}
# generating *fake* data to test the walk through- will be deleted ----
library(rgbif)
gbif_data <- occ_data(scientificName = "Asclepias eriocarpa", hasCoordinate = TRUE, limit = 400) 
new_data <- gbif_data$data %>%
  filter(stateProvince == "California",
         decimalLongitude < -121.0, 
         decimalLatitude < 37) %>%
  select(decimalLongitude, decimalLatitude, scientificName) %>%
  rename(longitude = decimalLongitude,
         latitude = decimalLatitude,
         milkweed_sp = scientificName) %>%
  mutate(milkweed_sp = "Asclepias eriocarpa",
         region = "North") %>%
  rename(scientific_name = milkweed_sp)

ggplot() +
  geom_point(data = new_data, aes(x = longitude, y = latitude), size = 0.5) +
  geom_sf(data = lpnf_north_buffered, alpha = 0.5)
```


#### Read in data used for modeling
```{r}
# environmental data raster stack
environmental_layers <- brick("path/to/env_stack.tif")
# environmental_layers <- envs_Ac

# los padres forest boundary files
lpnf_boundary <- st_read("path/to/lpnf_boundary.shp")
lpnf_north <- st_read("path/to/lpnf_boundary_north.shp")
lpnf_south <- st_read("path/to/lpnf_boundary_south.shp")

# read in properly formatted milkweed polygon data
milkweed_data_raw <- st_read("path/to/data")
# milkweed_data_raw <- st_read(here("~/../../capstone/milkweedmod/raw_data/milkweed_polygon_data/"))
```

##### Troubleshooting Errors
Problems with reading in the data:


# Species Distribution Model 
## Adding new data

### Clean new milkweed survey data
Now that the new data milkweed data is read in, we want to check that the structure of the data looks correct. Using `View(milkweed_data_raw)` look for the columns in the data `Milkweed_P`, `MilkweedSp`, `region`, and `geometry` (geometry will be the very last column). These are the columns we want to use for the species distribution modeling. 
```{r}
# view the data frame; the data frame will pop up in a new tab at the top
View(milkweed_data_raw)
```

##### Troubleshooting Errors
Different column names/missing columns:

### Clean New Data
If everything is looking good, let's move on to cleaning the data up a bit. We want to select only the columns we need for modeling, and rename them to names that are a bit simpler. Since this data has both "yes" and "no" values in the `Milkweed_P`, we want to filter to only keep the "yes" values. Finally, let's transform the coordinate reference system (CRS) to EPSG:4326, since this the the CRS that the prepared environmental data is in. 
```{r}
milkweed_clean <- milkweed_data_raw |> 
  janitor::clean_names() |> 
  filter(milkweed_p != "no")  %>%
  st_transform("EPSG:4326") %>%
  dplyr::select(milkweed_sp)

# check that the new clean data is looking correct
head(milkweed_clean)
```

Here's an example what the data structure should look like:
```
  milkweed_sp       geometry
1 Asclepias vestita MULTIPOLYGON (((-119.3003 3...
2 Asclepias erosa   MULTIPOLYGON (((-119.1852 3...
3 Asclepias cali... MULTIPOLYGON (((-119.402 34...
```

#### Convert Polygons to Points
Maxent modeling only works with point data, so we are going to use the multipolygon border to extract points from the area.

```{r}
# get the points from polygons
milkweed_cast_points <- st_cast(milkweed_clean, "MULTIPOINT") %>% 
  st_cast("POINT")
# here you will get a warning about sub-geometries, but that's okay

milkweed_points <- milkweed_cast_points %>%
  group_by(milkweed_sp) %>% 
  st_coordinates() %>% # get the lat long coords from the point geometries (this returns only the lat long as a list)
  data.frame() %>% # convert list to a data frame
  cbind(milkweed_cast_points) %>% # join the data frame of points with the casted geometry points to get the species 
  dplyr::select(-geometry) %>% # drop the casted geometry points (don't need since we have the lat long now)
  rename(longitude = X, # update names
         latitude = Y,
         scientific_name = milkweed_sp) %>% # in the function, the species column must be called 'scientific_name'
  mutate(occID = row_number())

head(milkweed_points)

# --------------------- for Sam--- delete
milkweed_points <- milkweed_cast_points %>%
  group_by(milkweed_sp) %>% 
  st_coordinates() %>% # get the lat long coords from the point geometries (this returns only the lat long as a list)
  data.frame() %>% # convert list to a data frame
  cbind(milkweed_cast_points) %>% # join the data frame of points with the casted geometry points to get the species 
  dplyr::select(-geometry) %>% # drop the casted geometry points (don't need since we have the lat long now)
  rename(longitude = X, # update names
         latitude = Y,
         scientific_name = milkweed_sp) %>%
  mutate(region = "South")
```

The head() of milkweed_points should look similar to this:
```
    longitude latitude       scientific_name occID
1   -119.9609 34.71440 Asclepias californica     1
1.1 -119.9610 34.71402 Asclepias californica     2
1.2 -119.9612 34.71343 Asclepias californica     3
```
*Note the index looks kind of weird (1, 1.1, 1.2 etc) since we split a polygon (one row) in to multiple row, which added the decimal steps. 


### Join new data with old data

If this new data is in the same data frame as the data originally used for modeling, **do not** run this next code chunk, and move on the the next section **Data Sub-setting**. If this new data has none of the previous survey data, continue with running the code below. 

```{r}
milkweed_points <- rbind(milkweed_points, milkweed_survey_2023) %>%
  mutate(occID = row_number())

# milkweed_points <- rbind(milkweed_points, new_data) %>%
#  mutate(occID = row_number())
```

##### Troubleshooting Errors


### Data subsetting

#### Select data for each species in the northern region of the LPNF
```{r}
# filter to only have data from the northern region 
milkweed_north <- milkweed_points %>%
  filter(region == "North")

# californica
californica_north <- milkweed_north %>%
  filter(scientific_name == "Asclepias californica")
# erosa
erosa_north <- milkweed_north %>%
  filter(scientific_name == "Asclepias erosa")
# eriocarpa
eriocarpa_north <- milkweed_north %>%
  filter(scientific_name == "Asclepias eriocarpa")
# vestita
vestita_north <- milkweed_north %>%
  filter(scientific_name == "Asclepias vestita")
```

#### Select data for each species in the southern region of the LPNF
```{r}
# filter to only have data from the southern region
milkweed_south <- milkweed_points %>%
  filter(region == "South")

# californica
californica_south <- milkweed_south %>%
  filter(scientific_name == "Asclepias californica")
# erosa
erosa_south <- milkweed_south %>%
  filter(scientific_name == "Asclepias erosa")
# eriocarpa
eriocarpa_south <- milkweed_south %>%
  filter(scientific_name == "Asclepias eriocarpa")
# vestita
vestita_south <- milkweed_south %>%
  filter(scientific_name == "Asclepias vestita")
```


## Model
To perform species distribution modeling, we have used various functions from the R packages {Wallace}, {dismo}, and {ENMeval}. There are many steps to the modeling process and these have been broken down into smaller sections


### Model South

This section......

Select the species you want to model
```{r}
## Change these values! ##
species_name <- "Asclepias eriocarpa" # change this to the species name
species_points_south <- eriocarpa_south # change this to the corresponding species data frame for the south
```

#### Obtain environmental data values based on the occurence coordinates and join with the occurance points

```{r}
occurence_coordinates_south <- species_points_south[c("longitude", "latitude")]
environmental_values_south <- as.data.frame(raster::extract(environmental_layers, occurence_coordinates_south, cellnumbers = TRUE))

# add columns for env variable values for each occurrence record
occurence_env_values_south <- cbind(species_points_south, environmental_values_south)
```

#### Process the occurence and environmental data
```{r}
# Spatially thin the occurrence points
species_points_south <- poccs_thinOccs(
  occs = occurence_env_values_south, 
  thinDist = 0.05) # adjust this value if you would like to change the thinning distaince (in km)


# Mask environmental data to provided extent
environmental_mask_south <- penvs_bgMask(
  occs = species_points_south, # occurence points
  envs = environmental_layers, # environmental layers
  bgExt = lpnf_south_buffered) # extent to model on (southern section of lpnf with a buffer)

# Sample background points from the provided area
bg_sample_points_south <- penvs_bgSample(
  occs = species_points_south, # occurence points
  bgMask =  environmental_mask_south, # environmental layers mask made above
  bgPtsNum = 5000) # number of points to be sampled from the area

# Extract values of environmental layers for each background point
bg_env_values_south <- as.data.frame(raster::extract(environmental_mask_south,  bg_sample_points_south))

#Add extracted values to background points table
bg_env_values_south <- cbind(scientific_name = paste0("bg_", species_name), bg_sample_points_south,
                            occID = NA, year = NA, institution_code = NA, country = NA,
                            state_province = NA, locality = NA, elevation = NA,
                            record_type = NA, bg_env_values_south)
```

#### Partition occurrence data

Partition occurrences and background points for model training and validation using block, a spatial partition method.

```{r}
# Partitioned data
partition_groups_south <- part_partitionOccs(
  occs = species_points_south ,
  bg =  bg_sample_points_south, 
  method = "block") 
```

#### Build and Evaluate Model

We are using “Linear” (L) and “Linear Quadratic” (LQ) feature class settings (where L is more simple, i.e., canopy cover + temperature, and LQ is more complex, i.e., canopy cover^2^ + temperature^2^) and not using Hinge (piecewise linear functions) because the relationship that we are modeling is relatively simple. 

Regularization multipliers penalize model complexity, where higher values indicate smoother, less complex models. This is used to retain only the variables with the greatest predictive contribution in the model, thus performing feature selection. Regularization multipliers from 0.5 to 4 by increments of 0.5 are used, resulting in 16 models (L, LQ, x1 each per incremental increase). 

```{r}
# Run maxent model for the selected species
species_model_south <- model_maxent(
  occs = species_points_south, 
  bg = bg_env_values_south,
  user.grp = partition_groups_south, 
  bgMsk = environmental_mask_south,
  rms = c(0.5, 4), 
  rmsStep =  0.5,
  fcs = c('L', 'LQ'),
  clampSel = FALSE,
  algMaxent = "maxnet",
  parallel = FALSE,
  numCores = 7)

# View table of model results
species_model_south@results %>%
  gt::gt()

# Select model and predict
auc_max_south <- species_model_south@results %>%
  select(tune.args, auc.train) %>%
  arrange(desc(auc.train)) %>%
  head(1) %>%
  mutate(tune.args = as.character(tune.args))

selected_model_south <- species_model_south@models[[auc_max_south$tune.args]] 

model_prediction_south <- predictMaxnet(selected_model_south, environmental_mask_south,
                                          type = "cloglog", # change for different types
                                          clamp = FALSE)
```

#### Model transfer (optional; only run if there is no North data)

If you have sufficient data for modeling separately on the Northern part of the forest, continue onto the next section: **Model North**. 

```{r}
# Generate a transfer of the model to the desired area
xfer_area_mod <- xfer_area(
  evalOut = species_model_south,
  curModel = auc_max_south$tune.args,
  envs = environmental_layers, 
  outputType = "cloglog",
  alg = "maxnet",
  clamp = FALSE,
  xfExt = lpnf_north_buffered)

# store the cropped transfer variables
xfer_prediction <- xfer_area_mod$xferArea

# join the south model and north transfer model
joined_model <- merge(xfer_prediction, model_prediction_south) %>%
  rast() %>%
  crop(lpnf_boundary, mask = TRUE)
```


### Model North

If you do not have sufficient data to model on the Northern part of the forest, do not attempt to run the code in the section. Instead, make sure you have run the section above: **Model Transfer**. 

The process for modeling on the Northern section is the same as modeling on the Southern section. For efficiency, the modeling steps have been combined into one code block with less comments throughout. 

Select the species you want to model
```{r}
## Change these values! ##
species_points_north <- eriocarpa_north # change this to the corresponding species data frame for the south
```

```{r}
# obtain the occurrence coordinates
occurence_coordinates_north <- species_points_north[c("longitude", "latitude")]
environmental_values_north <- as.data.frame(raster::extract(environmental_layers, occurence_coordinates_north, cellnumbers = TRUE))

# add columns for env variable values for each occurrence record
occurence_env_values_north <- cbind(species_points_north, environmental_values_north)

# Spatially thin the occurrence points
species_points_north <- poccs_thinOccs(
  occs = occurence_env_values_north, 
  thinDist = 0.05) # adjust this value if you would like to change the thinning distaince (in km)

# Mask environmental data to provided extent
environmental_mask_north <- penvs_bgMask(
  occs = species_points_north, # occurence points
  envs = environmental_layers, # environmental layers
  bgExt = lpnf_north_buffered) # extent to model on (northern section of lpnf with a buffer)

# Sample background points from the provided area
bg_sample_points_north <- penvs_bgSample(
  occs = species_points_north, # occurence points
  bgMask =  environmental_mask_north, # environmental layers mask made above
  bgPtsNum = 1000) # number of points to be sampled from the area

# Extract values of environmental layers for each background point
bg_env_values_north <- as.data.frame(raster::extract(environmental_mask_north,  bg_sample_points_north))

#Add extracted values to background points table
bg_env_values_north <- cbind(scientific_name = paste0("bg_", species_name), bg_sample_points_north,
                            occID = NA, year = NA, institution_code = NA, country = NA,
                            state_province = NA, locality = NA, elevation = NA,
                            record_type = NA, bg_env_values_north)

# Partitioned data
partition_groups_north <- part_partitionOccs(
  occs = species_points_north ,
  bg =  bg_sample_points_north, 
  method = "block") 

# Run maxent model for the selected species
species_model_north <- model_maxent(
  occs = species_points_north, 
  bg = bg_env_values_north,
  user.grp = partition_groups_north, 
  bgMsk = environmental_mask_north,
  rms = c(0.5, 4), 
  rmsStep =  0.5,
  fcs = c('L', 'LQ'),
  clampSel = FALSE,
  algMaxent = "maxnet",
  parallel = FALSE,
  numCores = 7)

# View table of model results
species_model_north@results %>%
  gt::gt()

# Select model and predict
auc_max_north <- species_model_north@results %>%
  select(tune.args, auc.train) %>%
  arrange(desc(auc.train)) %>%
  head(1) %>%
  mutate(tune.args = as.character(tune.args))

selected_model_north <- species_model_north@models[[auc_max_north$tune.args]] 

model_prediction_north <- predictMaxnet(selected_model_north, environmental_mask_north,
                                          type = "cloglog", # change for different types
                                          clamp = FALSE)
# join the south model and north model
joined_model <- merge(model_prediction_south, model_prediction_north) %>%
  rast() %>%
  crop(lpnf_boundary, mask = TRUE)
```


## Plotting
```{r}
# Get values of prediction
map_pred_values <- getRasterVals(joined_model, "cloglog") # change for different types

# Define colors and legend  
suitability_pal <- c("#FFFFFF", "#EFCCCC", "#DF9999", "#D06666", "#C03333", "#B00000")

legend_pal <- colorNumeric(suitability_pal, map_pred_values, na.color = 'transparent')
raster_pal <- colorNumeric(suitability_pal, map_pred_values, na.color = 'transparent')


leaflet() %>% 
  addProviderTiles(providers$Esri.WorldTerrain) %>%
  addLegend_decreasing("bottomleft", pal = legend_pal, values = map_pred_values,
            labFormat = reverseLabel(), decreasing = TRUE,
            title = paste(species_name, "<br>Predicted Suitability<br>")) %>%
  # Add model prediction
  addRasterImage(joined_model, colors = raster_pal, opacity = 0.7,
                 group = 'vis', layerId = 'mapPred', method = "ngb") %>%
  addPolygons(data = lpnf_boundary,
              fill = FALSE,
              color = "black",
              weight = 2)
```

## Save model outputs

```{r}
# install.packages(snakecase)
writeRaster(joined_model, paste0("outputs/sdm/", snakecase::to_snake_case(species_name), "_sdm.tif"))
```




# Updating “closed” Trails and Roads

**There is only the ability to filter roads by name or status in the southern section of the LPNF**

Navigate to `data_cleaning/trails_and_roads.qmd`. Scroll to line 114, the start of the section titled "Requested filtering option: Filter out specific trails and roads by name." There you will find a brief description of the following code and how to use it to filter out roads by their specific name and/or by their designated "open" or "closed" status in the data set. Follow these instructions, and run all the necesssary code to filter and save as cleaned data.

### Using those updates to update site accessibility 

Once the trails and roads data are filtered to your liking and saved in the `clean_data` folder, you have a few more steps to update the survey site accessibility index and priority score index.

1. First, navigate to `site_accessibility/distance_calculations.qmd`. Run this whole document to both re-calculate the distance from roads and trails with the updated data and re-save in the `clean_data` folder.
2. Then, navigate to `site_accessibiility/rescale_all_layers.qmd`. Run the whole document to both rescale the new distance dataset and re-save in the `clean_data` folder.
3. Now, navigate to `site_accessibiility/create_accessibility_index.qmd`. Run this whole document to update the survey site accessibility index and re-save it in the `outputs` folder.


Instruct on where/how to run rescale layers, etc. again.

## Updating Site Priority 

4. Lastly, navigate to the `priority_sites` folder and open both documents. First, run `priority_sites.qmd` to update the priority index for each milkweed species and re-save in the `outputs` folder. Then, run the `priority_sites_table.qmd` to update the table containing the locations of each raster cell, the priority score for each milkweed species, the accessibility score, and the visited status.
Using new outputs from SDM – north and south, species-specific and max(all) 

Save outputs to _____ (note here that this is where the dashboard will be pulling from) 


